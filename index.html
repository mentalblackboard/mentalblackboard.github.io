<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="button-row-dark">
    <a href="#" class="pill-dark">
      <span class="icon">âœ–</span> arXiv
    </a>
  
    <a href="#" class="pill-dark">
      <span class="icon">ðŸ“„</span> PDF
    </a>
  
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ’»</span> Code
    </a>
  
    <a href="https://huggingface.co/datasets/nlylmz/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ¤—</span> Huggingface
    </a>
  </div>

  <!-- Prediction -->
  <h3>Prediction</h3>
  <p>
    Predict the final hole configuration after unfolding,
    given the folding sequence and initial hole properties.
  </p>

  <div class="media-grid">
  <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Problem</p>
  </div>

  <div>
    <video controls playsinline>
      <source src="assets/videos/UP_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Solution</p>
  </div>
  </div>



  <!-- Backward Prediction -->
  <h3>Backward Prediction</h3>
  <p>
    Predict final hole configurations when folding actions
    occur away from the camera, introducing limited visual information.
  </p>

  <div class="media-grid">
   <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Problem</p>
    </div>

    <div>
    <video controls>
      <source src="assets/videos/UP_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Solution</p>
   </div>
  </div>


  <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <p>
    MentalBlackboard is built with an automated, physically
    grounded pipeline based on a 3D paper-folding and hole-punching simulation
    (VPython). Folding, rotation, punching, and unfolding are executed under
    strict physical constraints to avoid deformation or self-intersection, and
    the paper is discretized into triangular regions to support diagonal and
    multi-stage folds.
  </p>
     <img src="assets/figures/pipeline-1.png" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
  </section>

  <!-- DATASET -->
<section>
  <h2>Dataset</h2>

  <p>
    MentalBlackboard is an open-ended spatial visualization benchmark built
    on a physically grounded 3D simulation of paper folding and hole punching.
    The dataset supports symmetry- and rotation-based transformations and
    explicitly models the physical changes induced by rotations during
    folding and unfolding. It enables the generation of over
    12K unique folding configurations and more than
    1 million potential problem instances.
  </p>

  <p>
    The benchmark includes two core task settingsâ€”Prediction and Planningâ€”spanning
    video, 2D image, and text-based representations, along with a generalization
    task that evaluates spatial property transfer across analogous
    folding scenarios. MentalBlackboard adopts an open-ended evaluation framework
    to enable fine-grained analysis of spatial reasoning, symmetry application,
    and sequential mental transformations.
  </p>
<img src="assets/figures/last_tasks_cropped (2)-1.png" class="figure">
  <p class="caption">
       Examples of a prediction task and a planning task.
    </p>
</section>

  <!-- Evaluation Setup (Summary) -->
<section id="evaluation-setup">
  <h2>Evaluation Setup</h2>

  <p>
    We evaluate state-of-the-art Vision-Language Models in a
    zero-shot setting across video-, image-, and text-based
    prediction tasks, and 2D image-based planning tasks. Performance is measured
    using Exact Match and a custom Partial Accuracy metric designed for open-ended
    spatial predictions.
  </p>

  <p>
    Partial Accuracy accounts for both correct hole matches and over-prediction,
    and is defined as
    <em>M / (G + max(0, P âˆ’ G))</em>, where <em>G</em> is the number of
    ground-truth holes, <em>P</em> is the number of predicted holes, and
    <em>M</em> is the number of correctly matched holes. Field-wise accuracy is
    additionally reported for individual hole attributes to enable fine-grained
    analysis.
  </p>
</section>
  
  <!-- RESULTS -->
  <section>
    <h2>Main Results</h2>
    <ul>
    <li>
      State-of-the-art Vision-Language Models (VLMs) struggle with
      multi-stage symmetry transformations, particularly when
      reasoning over folded and unfolded paper structures.
    </li>
    <li>
      The best-performing models achieve at most
      25% exact match accuracy on prediction tasks, with
      substantially lower performance in image- and video-based settings.
    </li>
    <li>
      Planning tasks expose severe limitations in
      reverse spatial reasoning, with no model exceeding
      10% accuracy when inferring folding sequences from final
      hole patterns.
    </li>
    <li>
      Models are consistently more accurate at predicting
      <em>shape</em> and <em>size</em> than <em>location</em> and
      <em>direction</em>, indicating difficulties in spatial localization and
      physical orientation reasoning.
    </li>
    <li>
      Tasks involving rotation significantly degrade
      performance, suggesting limited understanding of the
      physical changes induced by reorientation.
    </li>
  </ul>
    <img src="assets/figures/table.jpg" class="figure">
    <p class="caption">
       Model performances across video-based, 2D image-based, and text-based prediction tasks.
    </p>
     <img src="assets/figures/planning.jpg" class="figure">
    <p class="caption">
      Performance of the models on the planning task: (a) partial accuracy per field; (b) exact match accuracy of
      baseline models.
    </p>
  </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
