<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="button-row-dark">
    <a href="#" class="pill-dark">
      <span class="icon">âœ–</span> arXiv
    </a>
  
    <a href="#" class="pill-dark">
      <span class="icon">ðŸ“„</span> PDF
    </a>
  
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ’»</span> Code
    </a>
  
    <a href="https://huggingface.co/datasets/nlylmz/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ¤—</span> Huggingface
    </a>
  </div>

  <!-- Prediction -->
  <h3>Prediction</h3>
  <p>
    Predict the final hole configuration after unfolding,
    given the folding sequence and initial hole properties.
  </p>

  <div class="media-grid">
  <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Problem</p>
  </div>

  <div>
    <video controls playsinline>
      <source src="assets/videos/UP_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Solution</p>
  </div>
  </div>



  <!-- Backward Prediction -->
  <h3>Backward Prediction</h3>
  <p>
    Predict final hole configurations when folding actions
    occur away from the camera, introducing limited visual information.
  </p>

  <div class="media-grid">
   <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Problem</p>
    </div>

    <div>
    <video controls>
      <source src="assets/videos/UP_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Solution</p>
   </div>
  </div>


  <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <p>
    MentalBlackboard is built with an automated, physically
    grounded pipeline based on a 3D paper-folding and hole-punching simulation
    (VPython). Folding, rotation, punching, and unfolding are executed under
    strict physical constraints to avoid deformation or self-intersection, and
    the paper is discretized into triangular regions to support diagonal and
    multi-stage folds.
  </p>
     <img src="assets/figures/pipeline-1.png" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
  </section>

  <!-- DATASET -->
<section>
  <h2>Dataset</h2>

  <p>
    MentalBlackboard is an open-ended spatial visualization benchmark built
    on a physically grounded 3D simulation of paper folding and hole punching.
    The dataset supports symmetry- and rotation-based transformations and
    explicitly models the physical changes induced by rotations during
    folding and unfolding. It enables the generation of over
    12K unique folding configurations and more than
    1 million potential problem instances.
  </p>

  <p>
    The benchmark includes two core task settingsâ€”Prediction and Planningâ€”spanning
    video, 2D image, and text-based representations, along with a generalization
    task that evaluates spatial property transfer across analogous
    folding scenarios. MentalBlackboard adopts an open-ended evaluation framework
    to enable fine-grained analysis of spatial reasoning, symmetry application,
    and sequential mental transformations.
  </p>
<img src="assets/figures/last_tasks_cropped (2)-1.png" class="figure">
  <p class="caption">
       Examples of a prediction task and a planning task.
    </p>
</section>

  <!-- Evaluation Setup (Summary) -->
<section id="evaluation-setup">
  <h2>Evaluation Setup</h2>

  <p>
    We evaluate state-of-the-art Vision-Language Models in a
    zero-shot setting across video-, image-, and text-based
    prediction tasks, and 2D image-based planning tasks. Performance is measured
    using Exact Match and a custom Partial Accuracy metric designed for open-ended
    spatial predictions.
  </p>

  <p>
    Partial Accuracy accounts for both correct hole matches and over-prediction,
    and is defined as
    <em>M / (G + max(0, P âˆ’ G))</em>, where <em>G</em> is the number of
    ground-truth holes, <em>P</em> is the number of predicted holes, and
    <em>M</em> is the number of correctly matched holes. Field-wise accuracy is
    additionally reported for individual hole attributes to enable fine-grained
    analysis.
  </p>
</section>
  
  <!-- RESULTS -->
  <section>
    <h2>Main Results</h2>
    <ul>
    <li>
      State-of-the-art Vision-Language Models (VLMs) struggle with
      multi-stage symmetry transformations, particularly when
      reasoning over folded and unfolded paper structures.
    </li>
    <li>
      The best-performing models achieve at most
      25% exact match accuracy on prediction tasks, with
      substantially lower performance in image- and video-based settings.
    </li>
    <li>
      Planning tasks expose severe limitations in
      reverse spatial reasoning, with no model exceeding
      10% accuracy when inferring folding sequences from final
      hole patterns.
    </li>
    <li>
      Models are consistently more accurate at predicting
      <em>shape</em> and <em>size</em> than <em>location</em> and
      <em>direction</em>, indicating difficulties in spatial localization and
      physical orientation reasoning.
    </li>
    <li>
      Tasks involving rotation significantly degrade
      performance, suggesting limited understanding of the
      physical changes induced by reorientation.
    </li>
  </ul>
    <img src="assets/figures/table.jpg" class="figure">
    <p class="caption">
       Model performances across video-based, 2D image-based, and text-based prediction tasks.
    </p>
     <img src="assets/figures/planning.jpg" class="figure">
    <p class="caption">
      Performance of the models on the planning task: (a) partial accuracy per field; (b) exact match accuracy of
      baseline models.
    </p>
  </section>

   <section>
     
    <h2>Do VLMs Transfer Spatial Information?</h2>
     <p>
    This section evaluates spatial knowledge transfer using a generalization task with 240
    2D image-based analogy questions across three folding configurations. Each task pairs
    two scenarios with identical folds but different hole data, requiring models to infer
    missing hole attributes by relating the two cases.
  </p>
  <p>
    The task isolates four hole attributesâ€”shape, size, location, and directionâ€”changing
    only one at a time. It measures spatial transfer rather than full spatial visualization or
    symmetry reasoning.
  </p>
     <img src="assets/figures/gen_cropped (2)-1.png" class="figure">
    <p class="caption">
      An example of a generalization task employs a visual analogical reasoning framework, which requires
      making an inference from the first scenario to identify the missing part in the second scenario.
    </p>
    <img src="assets/figures/generalization.jpg" class="figure">
    <p class="caption">
      Performance of the models on the generalization task: (a) exact match accuracy of baseline models; (b) the
    accuracy of the models for each category.
    </p>
    <p>
    Results show a large gap between proprietary and open-source models. Models perform
    best on shape and size, while direction and location remain challenging. Overall, models
    tend to underpredict holes. The identical folding patterns reduce the need for mental
    unfolding, explaining higher performance than in prediction or planning tasks.
    </p>
   </section>


   <section>
    <h2>How Do Representations Influence Performance?</h2>
    <p>
    This section studies the impact of representation format by comparing text-based,
    2D image-based, and video-based PFT prediction tasks after removing the direction
    attribute. Identical problem instances are used across modalities for fair comparison.
    Text-based representations generally yield higher accuracy than visual formats,
    especially for Claude models, which show large drops from text to image/video tasks.
    In contrast, GPT-4o exhibits nearly identical performance across all modalities, while
    GPT-5.1 shows only minor variation. These findings indicate that model performance is 
    strongly influenced by input representation, with some models relying more heavily on
     textual cues than visual ones.
  </p>
   </section>

   <section>
    <h2>How Does Backward Folding Affect Performance?</h2>
    <p>
    A backward folding prediction task is introduced using 180 video questions in which
    folding actions are reversed (away from the camera) while all other visual parameters
    remain unchanged. The final paper appearance matches that of forward-fold tasks. In the 
    backward folding task, reversing the folding direction while keeping the final paper appearance unchanged
    reveals clear model differences: o3 maintains performance comparable to forward folding, whereas the Claude
    models show reduced exact-match accuracy and generate more extra holes. These results indicate that some models,
    particularly the Claude variants, rely on view-dependent representations and struggle with spatial reasoning when
    the perspective of folding actions is reversed.
   </p>
   </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
