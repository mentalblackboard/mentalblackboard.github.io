<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="button-row-dark">
    <a href="#" class="pill-dark">
      <span class="icon">âœ–</span> arXiv
    </a>
  
    <a href="#" class="pill-dark">
      <span class="icon">ðŸ“„</span> PDF
    </a>
  
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ’»</span> Code
    </a>
  
    <a href="https://huggingface.co/datasets/nlylmz/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ¤—</span> Huggingface
    </a>
  </div>

  <!-- Prediction -->
  <h3>Prediction</h3>
  <p>
    Predict the final hole configuration after unfolding,
    given the folding sequence and initial hole properties.
  </p>

  <div class="media-grid">
  <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Problem</p>
  </div>

  <div>
    <video controls playsinline>
      <source src="assets/videos/UP_425_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Prediction â€” Solution</p>
  </div>
  </div>



  <!-- Backward Prediction -->
  <h3>Backward Prediction</h3>
  <p>
    Predict final hole configurations when folding actions
    occur away from the camera, introducing limited visual information.
  </p>

  <div class="media-grid">
   <div>
    <video controls autoplay muted loop playsinline>
      <source src="assets/videos/PF_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Problem</p>
    </div>

    <div>
    <video controls>
      <source src="assets/videos/UP_141_animation.mp4" type="video/mp4">
    </video>
    <p class="caption">Backward Prediction â€” Solution</p>
   </div>
  </div>

  <!-- Planning -->
  <h3>Planning</h3>
  <p>
    Infer the folding sequence and initial hole attributes
    from the final unfolded paper configuration.
  </p>

  <!-- Planning image -->
  <img src="assets/figures/planning_example.png" class="figure">


  <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <img src="assets/figures/pipeline-1.png" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
    <p>
    MentalBlackboard is built with an automated, physically
    grounded pipeline based on a 3D paper-folding and hole-punching simulation
    (VPython). Folding, rotation, punching, and unfolding are executed under
    strict physical constraints to avoid deformation or self-intersection, and
    the paper is discretized into triangular regions to support diagonal and
    multi-stage folds.
  </p>
  </section>

  <!-- DATASET -->
<section>
  <h2>Dataset</h2>

  <p>
    MentalBlackboard is an open-ended spatial visualization benchmark built
    on a physically grounded 3D simulation of paper folding and hole punching.
    The dataset supports symmetry- and rotation-based transformations and
    explicitly models the physical changes induced by rotations during
    folding and unfolding. It enables the generation of over
    12K unique folding configurations and more than
    1 million potential problem instances.
  </p>

  <p>
    The benchmark includes two core task settingsâ€”Prediction and Planningâ€”spanning
    video, 2D image, and text-based representations, along with a generalization
    task that evaluates spatial property transfer across analogous
    folding scenarios. MentalBlackboard adopts an open-ended evaluation framework
    to enable fine-grained analysis of spatial reasoning, symmetry application,
    and sequential mental transformations.
  </p>
<img src="assets/figures/last_tasks_cropped (2)-1.png" class="figure">
</section>
  
  <!-- RESULTS -->
  <section>
    <h2>Key Findings</h2>
    <img src="assets/figures/table.jpg" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
    <ul>
      <li>State-of-the-art VLMs struggle with multi-stage symmetry transformations.</li>
      <li>Best models achieve â‰¤ 25% accuracy on prediction tasks.</li>
      <li>Planning tasks reveal severe limitations in reverse spatial reasoning.</li>
    </ul>
  </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
