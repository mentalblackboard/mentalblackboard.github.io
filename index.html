<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="buttons">
    <a href="#" class="btn">ðŸ“„ Paper</a>
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="btn">ðŸ’» Code</a>
    <a href="#" class="btn">ðŸ“¦ Dataset</a>
  </div>



  <!-- TEASER VIDEO -->
  <section class="media-section">
    <video autoplay muted loop playsinline>
      <source src="assets/videos/teaser.mp4" type="video/mp4">
    </video>
    <p class="caption">
      Overview of MentalBlackboard tasks and foldingâ€“unfolding process.
    </p>
  </section>

  <!-- TASKS -->
<section>
  <h2>Tasks</h2>

  <!-- Prediction -->
  <h3>Prediction</h3>
  <p>
    Predict the final hole configuration after unfolding,
    given the folding sequence and initial hole properties.
  </p>

  <div class="media-grid">
    <video controls>
      <source src="assets/videos/prediction_example_1.mp4" type="video/mp4">
    </video>

    <video controls>
      <source src="assets/videos/prediction_example_2.mp4" type="video/mp4">
    </video>
  </div>

  <!-- Backward Prediction -->
  <h3>Backward Prediction</h3>
  <p>
    Predict final hole configurations when folding actions
    occur away from the camera, introducing limited visual information.
  </p>

  <div class="media-grid">
    <video controls>
      <source src="assets/videos/backward_prediction_1.mp4" type="video/mp4">
    </video>

    <video controls>
      <source src="assets/videos/backward_prediction_2.mp4" type="video/mp4">
    </video>
  </div>

  <!-- Planning -->
  <h3>Planning</h3>
  <p>
    Infer the folding sequence and initial hole attributes
    from the final unfolded paper configuration.
  </p>

  <!-- Planning image -->
  <img src="assets/figures/planning_example.png" class="figure">

  <!-- Planning videos -->
  <div class="media-grid">
    <video controls>
      <source src="assets/videos/planning_example_1.mp4" type="video/mp4">
    </video>

    <video controls>
      <source src="assets/videos/planning_example_2.mp4" type="video/mp4">
    </video>
  </div>

</section>

    <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <img src="assets/figures/pipeline.pdf" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
    <div class="image-grid-3">
    <img src="assets/figures/fold_type_grouped_pie_chart (1).png">
    <img src="assets/figures/prediction_heatmap.png">
    <img src="assets/figures/shape_category_pie_chart_gradient_no_bold_cropped (1).png">
</div>

  </section>

  <!-- RESULTS -->
  <section>
    <h2>Key Findings</h2>
    <img src="assets/figures/table.jpg" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
    <ul>
      <li>State-of-the-art VLMs struggle with multi-stage symmetry transformations.</li>
      <li>Best models achieve â‰¤ 25% accuracy on prediction tasks.</li>
      <li>Planning tasks reveal severe limitations in reverse spatial reasoning.</li>
    </ul>
  </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
