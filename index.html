<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="button-row-dark">
    <a href="#" class="pill-dark">
      <span class="icon">âœ–</span> arXiv
    </a>
  
    <a href="#" class="pill-dark">
      <span class="icon">ðŸ“„</span> PDF
    </a>
  
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ’»</span> Code
    </a>
  
    <a href="https://huggingface.co/datasets/nlylmz/MentalBlackboard" class="pill-dark">
      <span class="icon">ðŸ¤—</span> Huggingface
    </a>
  </div>


 <!-- TEASER -->
<section class="teaser-compact">

  <h2>Teaser</h2>

  <!-- TASK TOGGLE -->
  <div class="toggle-row">
    <input type="radio" name="task" id="task-pred" checked>
    <label for="task-pred">Prediction</label>

    <input type="radio" name="task" id="task-back">
    <label for="task-back">Backward Prediction</label>
  </div>

  <!-- MODE TOGGLE -->
  <div class="toggle-row">
    <input type="radio" name="mode" id="mode-problem" checked>
    <label for="mode-problem">Problem</label>

    <input type="radio" name="mode" id="mode-solution">
    <label for="mode-solution">Solution</label>
  </div>

  <!-- VIDEO STACK -->
  <div class="video-switch">

    <!-- Prediction -->
    <video class="v pred problem" autoplay muted loop playsinline>
      <source src="assets/videos/prediction_problem.mp4" type="video/mp4">
    </video>

    <video class="v pred solution" autoplay muted loop playsinline>
      <source src="assets/videos/prediction_solution.mp4" type="video/mp4">
    </video>

    <!-- Backward Prediction -->
    <video class="v back problem" autoplay muted loop playsinline>
      <source src="assets/videos/backward_problem.mp4" type="video/mp4">
    </video>

    <video class="v back solution" autoplay muted loop playsinline>
      <source src="assets/videos/backward_solution.mp4" type="video/mp4">
    </video>

  </div>

  <p class="caption">
    Toggle between tasks and problem/solution views for MentalBlackboard.
  </p>

</section>

  <!-- Prediction -->
  <h3>Prediction</h3>
  <p>
    Predict the final hole configuration after unfolding,
    given the folding sequence and initial hole properties.
  </p>

  <div class="media-grid">
    <video controls>
      <source src="assets/videos/PF_425_animation.mp4" type="video/mp4">
    </video>

    <video controls>
      <source src="assets/videos/UP_425_animation.mp4" type="video/mp4">
    </video>
  </div>

  <!-- Backward Prediction -->
  <h3>Backward Prediction</h3>
  <p>
    Predict final hole configurations when folding actions
    occur away from the camera, introducing limited visual information.
  </p>

  <div class="media-grid">
    <video controls>
      <source src="assets/videos/PF_141_animation.mp4" type="video/mp4">
    </video>

    <video controls>
      <source src="assets/videos/UP_141_animation.mp4" type="video/mp4">
    </video>
  </div>

  <!-- Planning -->
  <h3>Planning</h3>
  <p>
    Infer the folding sequence and initial hole attributes
    from the final unfolded paper configuration.
  </p>

  <!-- Planning image -->
  <img src="assets/figures/planning_example.png" class="figure">


  <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <img src="assets/figures/pipeline-1.png" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
  </section>

   <!-- DATASET -->
<section>
  <h2>Tasks</h2>
  <img src="assets/figures/last_tasks_cropped (2)-1.png" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
</section>

  <!-- RESULTS -->
  <section>
    <h2>Key Findings</h2>
    <img src="assets/figures/table.jpg" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
    <ul>
      <li>State-of-the-art VLMs struggle with multi-stage symmetry transformations.</li>
      <li>Best models achieve â‰¤ 25% accuracy on prediction tasks.</li>
      <li>Planning tasks reveal severe limitations in reverse spatial reasoning.</li>
    </ul>
  </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
