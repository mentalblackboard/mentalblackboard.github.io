<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MentalBlackboard</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="container">

  <!-- TITLE -->
  <h1 class="title">MentalBlackboard</h1>
  <p class="subtitle">
    Evaluating Spatial Visualization via Mathematical Transformations
  </p>

  <!-- BUTTONS -->
  <div class="buttons">
    <a href="#" class="btn">ðŸ“„ Paper</a>
    <a href="https://github.com/mentalblackboard/MentalBlackboard" class="btn">ðŸ’» Code</a>
    <a href="#" class="btn">ðŸ“¦ Dataset</a>
  </div>



  <!-- TEASER VIDEO -->
  <section class="media-section">
    <video autoplay muted loop playsinline>
      <source src="assets/videos/teaser.mp4" type="video/mp4">
    </video>
    <p class="caption">
      Overview of MentalBlackboard tasks and foldingâ€“unfolding process.
    </p>
  </section>

  <!-- TASKS -->
  <section>
    <h2>Tasks</h2>

    <div class="media-grid">
      <div>
        <video controls>
          <source src="assets/videos/prediction_example.mp4" type="video/mp4">
        </video>
        <p class="caption"><b>Prediction:</b> Predict final hole configuration after unfolding.</p>
      </div>

      <div>
        <video controls>
          <source src="assets/videos/planning_example.mp4" type="video/mp4">
        </video>
        <p class="caption"><b>Planning:</b> Infer folding sequence from final hole pattern.</p>
      </div>

      <div>
        <video controls>
          <source src="assets/videos/generalization_example.mp4" type="video/mp4">
        </video>
        <p class="caption"><b>Generalization:</b> Transfer spatial relations via visual analogy.</p>
      </div>
    </div>
  </section>

    <!-- ABSTRACT -->
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      MentalBlackboard is a large-scale, open-ended benchmark designed to evaluate
      spatial visualization abilities in Vision-Language Models.
      The benchmark extends paper folding and hole punching tests with symmetry
      and rotation transformations in a physically grounded 3D environment.
      It introduces prediction, planning, and generalization tasks across
      video, image, and text modalities.
    </p>
  </section>

  <!-- PIPELINE -->
  <section>
    <h2>Dataset Generation</h2>
    <img src="assets/figures/pipeline.pdf" class="figure">
    <p class="caption">
      Automated VPython-based pipeline for folding, rotation, punching, and unfolding.
    </p>
  </section>

  <!-- RESULTS -->
  <section>
    <h2>Key Findings</h2>
    <ul>
      <li>State-of-the-art VLMs struggle with multi-stage symmetry transformations.</li>
      <li>Best models achieve â‰¤ 25% accuracy on prediction tasks.</li>
      <li>Planning tasks reveal severe limitations in reverse spatial reasoning.</li>
    </ul>
  </section>

  <!-- FOOTER -->
  <footer>
    <p>Â© 2026 MentalBlackboard Project</p>
  </footer>

</div>

</body>
</html>
